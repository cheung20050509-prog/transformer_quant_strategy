@inproceedings{liu2024itransformer,
  title={iTransformer: Inverted Transformers Are Effective for Time Series Forecasting},
  author={Liu, Yong and Hu, Tengge and Zhang, Haoran and Wu, Haixu and Wang, Shiyu and Ma, Lintao and Long, Mingsheng},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2024}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@article{zhang2020transformer_stock,
  title={Transformer-based attention network for stock movement prediction},
  author={Zhang, Qiuyue and Qin, Chao and Zhang, Wei and Lin, Fangzhen and Chen, Ling},
  journal={Expert Systems with Applications},
  volume={202},
  pages={117239},
  year={2022},
  publisher={Elsevier}
}

@article{ding2020hierarchical,
  title={Hierarchical multi-scale Gaussian transformer for stock movement prediction},
  author={Ding, Qianggang and Wu, Sifan and Sun, Hao and Guo, Jiadong and Guo, Jian},
  journal={Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence},
  pages={4640--4646},
  year={2020}
}

@article{kelly1956new,
  title={A new interpretation of information rate},
  author={Kelly, John L},
  journal={The Bell System Technical Journal},
  volume={35},
  number={4},
  pages={917--926},
  year={1956}
}

@article{sharpe1966mutual,
  title={Mutual fund performance},
  author={Sharpe, William F},
  journal={The Journal of Business},
  volume={39},
  number={1},
  pages={119--138},
  year={1966}
}

@article{akiba2019optuna,
  title={Optuna: A next-generation hyperparameter optimization framework},
  author={Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
  journal={Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={2623--2631},
  year={2019}
}

@article{wu2021autoformer,
  title={Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting},
  author={Wu, Haixu and Xu, Jiehui and Wang, Jianmin and Long, Mingsheng},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={22419--22430},
  year={2021}
}

@article{hochreiter1997lstm,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural Computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997}
}

@article{nie2023patchtst,
  title={A time series is worth 64 words: Long-term forecasting with transformers},
  author={Nie, Yuqi and Nguyen, Nam H and Sinthong, Phanwadee and Kalagnanam, Jayant},
  journal={International Conference on Learning Representations (ICLR)},
  year={2023}
}

@article{zhou2021informer,
  title={Informer: Beyond efficient transformer for long sequence time-series forecasting},
  author={Zhou, Haoyi and Zhang, Shanghang and Peng, Jieqi and Zhang, Shuai and Li, Jianxin and Xiong, Hui and Zhang, Wancai},
  journal={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={12},
  pages={11106--11115},
  year={2021}
}

@article{fischer2018deep,
  title={Deep learning with long short-term memory networks for financial market predictions},
  author={Fischer, Thomas and Krauss, Christopher},
  journal={European Journal of Operational Research},
  volume={270},
  number={2},
  pages={654--669},
  year={2018}
}

@book{murphy1999technical,
  title={Technical Analysis of the Financial Markets: A Comprehensive Guide to Trading Methods and Applications},
  author={Murphy, John J},
  year={1999},
  publisher={New York Institute of Finance}
}

@article{arlot2010survey,
  title={A survey of cross-validation procedures for model selection},
  author={Arlot, Sylvain and Celisse, Alain},
  journal={Statistics Surveys},
  volume={4},
  pages={40--79},
  year={2010}
}
